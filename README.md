# 100-days-of-Machine-Learning

# Day1 
Day 1 of learning Numpy! Numpy is a powerful Python library used for numerical computing. It provides a fast and efficient way to work with arrays, and is widely used in the fields of data science, machine learning, and scientific computing.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Numpy)

# Day2 and 3 
Day 2 and 3 of learning Pandas! Pandas is a powerful Python library used for data manipulation and analysis. It provides fast, flexible, and expressive data structures designed to make working with "relational" or "labeled" data both easy and intuitive Refer book - https://github.com/shyamkumarchauhan/learn-python-with-shyam/blob/master/cheatsheet/Pandas-Cookbook-eBook.pdf.

# Day4

Some More Pandas advanced concepts multiindexing,pivottable, datetime concepts Refer to my blog on pivot table : https://medium.com/@tahera-firdose/exploring-data-with-pivot-tables-in-pandas-a-practical-guide-with-examples-from-the-titanic-938f65f64285

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Pandas)

# Day 5 
data visualization using the Matplotlib library in Python.

Matplotlib is a powerful data visualization library widely used by data scientists, analysts, and researchers to create informative and appealing visualizations. It provides a range of tools for creating various types of plots, including line charts, bar charts, scatter plots, histograms, heatmaps, and more.

The examples in this repository cover different use cases and demonstrate how to use Matplotlib to create effective and informative visualizations. The examples include:

Line Chart: A simple example of creating a line chart using Matplotlib.

Bar Chart: An example of creating a bar chart to compare data across different categories.

Scatter Plot: An example of creating a scatter plot to visualize the relationship between two variables.

Histogram: An example of creating a histogram to visualize the distribution of a dataset.

Heatmap: An example of creating a heatmap to visualize the correlation between variables in a dataset.
 
 [code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Matplotlib.ipynb)
 
 [part2](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Matplotlib%20Part2.ipynb)
# Day 6

Data Visualization using  Plotly, which is a powerful data visualization library in Python. Plotly allows us to create interactive and dynamic visualizations, making it a great tool for exploratory data analysis and storytelling.

I explore the various features and capabilities of Plotly and learnt how to create basic plots such as line plots, bar charts, scatter plots, and pie charts. Additionally, explored more advanced visualizations like heatmaps, 3D plots, and animated plots.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Plotly.ipynb)

# Day 7,8,9 

During these three days, I covered a range of SQL concepts and techniques. I started with the basics, such as creating tables, inserting data, and querying data using the SELECT statement and then progress to more advanced topics, including joins, aggregations, subqueries, and data manipulation.

I also solved some question from Leetcode. Refer to PDF for the database questions with solutions

Also followed https://www.youtube.com/@techTFQ and learnt some important concepts like subqueries, joins, windows and normalization.

# Day 10 - Data Visualiation using Tableau

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9063973e-70d4-4781-8962-a5b812e62290)

# Day 11

Feature Scaling - Learnt Feature scaling - Standardization and Normalization methods
Refer to Blog - https://medium.com/p/122bd97cdbfc

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Feature%20Scaling)
# Day 12

Categorical Data encoding - Understood the below encoding techniques for Categorical data
1. OneHot Encoding or get dummies -Used to convert Nominal categorical data to Numeric data
2. Ordinal Encoder - Used to convert Ordinal categorical data to Numeric data
3. Label Encoder - Used to convert target variables from Categorical to Numericcal Data

Please refer to https://www.linkedin.com/feed/update/urn:li:activity:7067400868047794177/ for a complete detail analysis of the categoricaal data with examples

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Categorical%20data%20Encoding)

# Day 13

Column Transformer and Pipeline
![comparision_61645596](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/92a0becd-734e-4938-8bfe-47600b309a06)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Column%20Transformer%20and%20Pipelines)

# Day 14 - Handling missing values using dropna
Handling Missing Values with dropna: I explored into the importance of addressing missing values and explore the dropna method as a popular solution. I discuss the concept of missing values, their consequences, and how the dropna method can simplify data preprocessing.

Refer to blog for complete analysis https://tahera-firdose.medium.com/the-importance-of-handling-missing-values-an-overview-of-the-dropna-method-c70795c04c2c

# Day 15 - Handling missing values using mean and median
Missing values can hinder accurate analysis and modeling, but by filling them with the mean and median, we can preserve the overall distribution of the data. In this blog post, I dive into the process, advantages, and limitations of this approach, and demonstrate how to implement it using Python.

https://www.linkedin.com/feed/update/urn:li:activity:7068816645347049472/

# Day16 - Explored Most Frequent Imputation and Missing Category Imputation for Categorical data

Most Frequent Imputation: replacing missing values with the most frequently occurring category in the variable.
Missing Category Imputation: assigning a separate category to represent missing values.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/Handling%20Categrical%20Data.ipynb)

# Day17 - Random Value Imputation

Random imputation involves replacing missing values with randomly selected values from the existing dataset. This approach is useful when the missingness is random and there is no specific pattern or correlation to the missing values. Please refer to https://www.linkedin.com/feed/

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/Random%20Imputation.ipynb)

# Day 18 - KNN Imputation

KNN Imputation: KNN imputation is a method used to fill in missing values by estimating them from neighboring data points. It is called multivariate because it takes into account multiple variables or features in the dataset to estimate missing values. By considering the values of other variables, KNN imputation leverages relationships and patterns within the data to impute missing values effectively.

For more details please refer to https://medium.com/@tahera-firdose/knn-imputation-an-effective-approach-for-handling-missing-data-5c8bbb45c81a

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/KNN%20Imputation.ipynb)

# Day 19 and 20

Outliers : Outliers are data points that significantly deviate from the majority of the dataset, potentially indicating unusual or erroneous observations. Handling outliers is crucial in data analysis and modeling to ensure accurate insights and reliable results.

![istockphoto-515066055-1024x1024](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/6bcb60df-9bc9-4f48-a4b8-873a2b43a7a7)

Refer to https://www.linkedin.com/feed/update/urn:li:activity:7070132016687558657/ for complete details.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Handling%20Outliers)

# Day 21 - Learnt key concepts about simple linear regression including the Best Fit Line, Slope, Intercept, and Residual Error.

üìà What is Simple Linear Regression?
Simple Linear Regression is a fundamental technique in statistics and machine learning used to establish a relationship between two variables: the dependent variable (y) and the independent variable (x). It aims to find the best fit line that represents the relationship between these variables.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/7614ce9e-b26f-4c55-a1b6-319931e18cce)


üìâ Best Fit Line:
The Best Fit Line, also known as the Regression Line, is the line that minimizes the overall distance between the observed data points and the predicted values. It serves as a visual representation of the relationship between the independent and dependent variables.

üìè Slope:
The Slope (m) of the Best Fit Line measures the rate of change of the dependent variable (y) with respect to the independent variable (x). It quantifies the direction and steepness of the line. A positive slope indicates a positive correlation, while a negative slope represents a negative correlation between the variables.

üéØ Intercept:
The Intercept (b) is the point where the Best Fit Line intersects the y-axis. It represents the predicted value of the dependent variable (y) when the independent variable (x) is zero. The intercept helps determine the initial starting point of the line.

üîç Residual Error:
Residual Error, also referred to as Residuals, represents the difference between the observed values and the predicted values on the Best Fit Line. It quantifies how well the regression line fits the data points. Minimizing the residual error is a crucial aspect of Simple Linear Regression.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/Simple%20LInear%20Regression%20using%20Scikit%20learn.ipynb)

# Day 22 and 23 - Understood the Geometric intuition of best fit line using OLS.

Please refer to https://tahera-firdose.medium.com/linear-regression-derivation-of-slope-and-intercept-using-ordinary-least-square-971534ec6b77 for the derivation of Slope and Intercept using OLS.

Also coded the Linear Regression class from scratch and compared the results with ScikitLearn Linear Regression class
[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/LinearRegression%20from%20Scratch.ipynb)

# Day 24 - Learnt Evaluation Metrics used in Linear Regression

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9005856d-2ce4-453a-8579-afaae56d050d)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/f35248cc-b14f-43e6-98ea-b20f1a2b8216)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/6427ebb5-e750-4e62-b447-d66fa3908122)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/c5867b2e-04e4-4f14-b3ef-1bcb9bc7a9b4)

Please refer to blog https://medium.com/@tahera-firdose/understanding-regression-evaluation-metrics-244a106ac30f

[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/Evaluation%20Metricsipynb)

# Day 25 - Multiple Linear Regression

Multiple linear regression is an extension of simple linear regression, where we aim to model the linear relationship between multiple independent variables and a dependent variable. It assumes that the relationship between the independent variables and the dependent variable is additive, meaning each independent variable contributes independently to the dependent variable.

The Equation:
In multiple linear regression, we express the relationship between the independent variables (x1, x2, ..., xn) and the dependent variable (y) through the following equation:

y = Œ≤0 + Œ≤1 * x1 + Œ≤2 * x2 + ... + Œ≤n * xn + Œµ

Here, y represents the dependent variable, Œ≤0 is the intercept, Œ≤1 to Œ≤n are the coefficients corresponding to the independent variables, x1 to xn, and Œµ represents the error term

Estimating Coefficients:
The coefficients (Œ≤0, Œ≤1, Œ≤2, ..., Œ≤n) in the multiple linear regression equation are estimated using various statistical techniques. The most common method is Ordinary Least Squares (OLS), which minimizes the sum of squared differences between the observed and predicted values.

Interpreting Coefficients:
The coefficients in multiple linear regression indicate the change in the dependent variable corresponding to a one-unit change in the respective independent variable, holding other variables constant. Positive coefficients suggest a positive relationship, while negative coefficients suggest a negative relationship.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9cbfed98-a7e8-46cf-b0b9-f6018ca5c87d)



[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Multiple%20Linear%20Regression)

# Day 26 and Day 27 - Gradient Descent

Gradient descent is an iterative process that helps us find the minimum or maximum point of a function. The "gradient" refers to the slope or direction of change of the function, much like the steepest downhill direction on a hilly terrain. The "descent" part means moving in the direction opposite to the gradient, taking small steps toward the bottom of the hill. By repeatedly adjusting our position based on the negative gradient, we gradually approach the minimum (or maximum) point, making our way down the slope and finding the optimal value of the function.


![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/f901b703-f278-4e13-88e5-1f630a96b988)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Gradient%20Descent) 

# Day 28 and Day 29 - Learnt About different types of Gradient Descent

**Batch Gradient Descent (BGD)** computes the gradient using the entire training dataset, ensuring stability but being computationally expensive for large datasets.

![Batch G](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/87f007c1-b4d1-4978-84f0-37f6bcb61031)

**Stochastic Gradient Descent (SGD)** takes it further by using one training example at a time, offering efficiency and the potential to find better solutions but with more erratic convergence.

![Stochastic G](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/a5bc1368-c3d9-4600-bb03-23ce2031b555)

**Mini-Batch Gradient Descent (MBGD)** strikes a balance by using subsets of the data, providing faster convergence and parallelization.

![Mini Batch G](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/7847440a-fb62-4484-b72a-716d80b9fb79)

# Day 30 Polynomial Regression

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/5a6941cd-7e79-48e6-8760-70a6b615dd4d)

Refer to blog https://tahera-firdose.medium.com/understanding-polynomial-regression-603eb25501d

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Polynomial%20Regression)

# Day 31 Local and Global Minima

Local Minima: In the context of optimization, a local minimum refers to a point or value in a function where the function reaches the lowest value within a specific region but may not be the lowest value across the entire function.

Global Minima: In optimization, a global minimum is the absolute lowest point or value in a function, meaning it is the lowest value across the entire function domain. It represents the global "valley" or the optimal solution that yields the lowest possible value.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/881f4483-82f4-458a-a041-ccb91a7f3f7a)

![local_minima_vs_global_minima](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/991dc121-de3f-4993-9403-eaa90b5c95cf)

# Day 32 - Overfitting, Underfitting, Bias Variance Tradeoff

Finding the optimal balance between the bias and Variance  is the key to unlocking model performance and generalization. Refer to blog for enhancing your understanding of this fundamental concept in AI.
https://medium.com/@tahera-firdose/bias-variance-tradeoffs-61ea08a25f6f

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/e2387f33-ba6b-4224-b1ab-b321692fe5db)

# Day 33 Ridge Regression using OLS

 Ridge regression modifies the OLS estimation by introducing a penalty term that controls the magnitude of the coefficients. This tradeoff between fitting the data and shrinking the coefficients helps to find a balance between model complexity and generalization, making Ridge regression a useful technique for handling multicollinearity and reducing model variance.

 ![Ridge-regression-variable-selection](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/d2e340aa-a5a4-4019-a3e3-d85c75fb7152)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Ridge%20Regression/Ridge%20Regression%20-%20OLS.ipynb) 
 # Day 34 Ridge Regression using Gradient Descent

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/81564254-7bf1-481c-8371-92f42298cd9d)

Ridge regression using gradient descent is an iterative optimization algorithm that iteratively updates the coefficients by taking steps in the direction of the steepest descent of the objective function. It is a numerical approximation method that can handle large datasets and high-dimensional problems more efficiently. Gradient descent allows for fine-tuning the learning rate and regularization parameter to control the convergence speed and balance between fitting the data and regularization.

The key steps involved in Ridge regression using gradient descent are as follows:

Initialize the coefficients.

Calculate the gradient of the objective function with respect to the coefficients.

Update the coefficients by taking steps in the direction of the negative gradient, multiplied by the learning rate.

Repeat steps 2 and 3 until convergence or a maximum number of iterations is reached.

Ridge regression using gradient descent offers more flexibility and scalability compared to the OLS approach. It allows for efficient optimization and can handle larger and more complex datasets. However, it is an approximation method and might require careful tuning of hyperparameters such as the learning rate and regularization parameter to achieve optimal results.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Ridge%20Regression/Ridge%20_%20Gradient%20Descent.ipynb)

# Day 35 Lasso L1 Regression  

Lasso (Least Absolute Shrinkage and Selection Operator) is a regression technique that performs feature selection and regularization by shrinking less important feature coefficients to zero, leading to sparsity in the model.

Refer to blog for detailed understanding

https://tahera-firdose.medium.com/lasso-regression-a-comprehensive-guide-to-feature-selection-and-regularization-2c6a20b61e23

# Day 36 Elastic Net regression

Elastic Net regression is a powerful technique that combines the benefits of L1 (Lasso) and L2 (Ridge) regularization methods. It addresses some of the limitations of these individual regularization techniques and offers a flexible approach for handling high-dimensional datasets with potential multicollinearity issues.

The mathematical formulation of Elastic Net regression can be represented as follows:

Minimize:

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/041a0ca3-c80d-4193-8b6d-59bbc72a1d73)

Refer hto blog to learn more on ttps://www.linkedin.com/advice/0/what-some-common-pitfalls-challenges-elastic#:~:text=What%20is%20elastic%20net%20regression,by%20a%20parameter%20called%20alpha.

# Day 37 Logistic Regression - Sigmoid Function

Please refer to blog to understand the sigmoid function 

https://www.learndatasci.com/glossary/sigmoid-function/

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/1d6766f7-a3a5-43bf-b343-02dd2ad09f6b)

# Day 38 Logistic Regression - Loss Functiion

Refer to blog to understand the loss function for Logistic regresion

https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11

# Day 39 Log Loss error

Refer to blog for details
https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a

# Day 40 Binary Classification 
Binary classification is a type of machine learning task that involves categorizing data into one of two classes or categories. The goal of binary classification is to develop a model that can learn from labeled examples and make predictions on new, unseen examples.

In binary classification, the two classes are often referred to as the positive class and the negative class, or class 1 and class 0. The model learns patterns and relationships in the input data to determine which class a new example belongs to. The input data is typically represented by a set of features or attributes that describe each example.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/31ec4432-5822-4366-9f35-c86b1f53b249)

Refer to blog: https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/

# Day 41 Precision, Recall, F1 score, Confusion Matrix

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/5642ff2a-b62c-44cd-8b11-f361ed6a3d23)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/97f3a840-16dd-4167-aae1-4ec6b3d2331d)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9fd75220-850e-45c2-a2b7-c733de8b31ee)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/a3b96f2a-ae5a-48ab-a8ed-f8561b943fa9)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/83cd0066-7992-4c94-9381-b96a202bfa87)


Please refer to blog to gain the insights on this topic

https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-

https://proclusacademy.com/blog/explainer/precision-recall-f1-score-classification-models/

https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/

# Day 42 - Multiclass - Softmax regression

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/49456713-c1dc-4da4-b0de-dce9661be301)

Refer to video https://www.youtube.com/watch?v=hYBwBmojXoU to learn about Softmax Regression

# Day 43 - Decision Tree
Learnt what is Decision Tree, Entropy and Informational Gain

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/dd039e94-b2bc-43fe-9b50-4b7e8abd8193)

Refer to below to learn about Decision Tree

 https://blog.paperspace.com/decision-trees/
 
 https://www.saedsayad.com/decision_tree.htm

 # Day 44 - Decision Tree - Gini Impurity

A measurement used to build Decision Trees to determine how the features of a dataset should split nodes to form the tree.

Refer to below article to learn about Gini Impurity

 https://www.learndatasci.com/glossary/gini-impurity/

# Day 45 Decision Tree Hyperparameters

Decision tree hyperparameters are settings that control the behavior and performance of a decision tree algorithm. These parameters are set by the user before training the decision tree model and can greatly influence its effectiveness. 

https://ken-hoffman.medium.com/decision-tree-hyperparameters-explained-49158ee1268e

https://inria.github.io/scikit-learn-mooc/python_scripts/trees_hyperparameters.html

# Day 46 Curse Of Dimensionality

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/b1fdeead-1b89-43e9-aca2-0ef1b53d2819)

https://www.youtube.com/watch?v=L9eNxU-9jBQ

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/961f1fba-8edc-428f-b3d0-21cbcf79b09e)

# Day 47 Cross Validation

Cross-validation is a model evaluation technique that addresses the need to assess a model‚Äôs performance on unseen data. It involves dividing the dataset into multiple subsets or ‚Äúfolds‚Äù and training the model on some folds while validating it on others.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/d08c3a6f-ae3e-4306-b3e3-3c0c17423dce)

[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Cross%20Validation)

# Day 48 Ensemble Learning
Imagine you have a tough decision to make and you seek advice from several experts. Ensemble learning follows a similar principle. Instead of relying on a single machine learning model, ensemble learning combines predictions from multiple models to make a more accurate and robust prediction.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/ce0c2d32-41d2-4ec4-9feb-058054d4c9b4)

# Day 49 Voting Classifiers

Voting and averaging are two of the easiest examples of ensemble learning in machine learning. They are both easy to understand and implement. Voting is used for classification and averaging is used for regression.

refer to blog https://tahera-firdose.medium.com/maximizing-model-performance-with-voting-ensemble-learning-290a299769d1 to learn more in detail

# Day 50 Continuation of Voting Classifier (Hard and Soft Voting)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/3c8d39d6-903e-420f-96ec-d2ce80bda381)

# Day 51 and 52 - Bagging

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9c23358e-14e3-4da7-a9da-0bb7fc45d5cf)

Bagging ‚Äî a technique that encapsulates the wisdom of the crowd by aggregating predictions from multiple models. The name ‚ÄúBootstrap Aggregating‚Äù reveals its two key components:

Bootstrap: A statistical technique involving drawing random samples from a dataset with replacement. This process creates diverse training subsets, each with its unique perspective.
Aggregating: Combining the outputs of different models to make a final decision. For classification tasks, aggregation often involves majority voting.

To read more click on (https://medium.com/@tahera-firdose/unveiling-the-ensemble-enigma-bagging-your-way-to-supercharged-predictions-e1117a77bf50)

# Day 53 - Random Forest Classifier

Random forest is a commonly-used machine learning algorithm trademarked by Leo Breiman and Adele Cutler, which combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/f70cd6c3-4aa8-4820-ab43-84019f5f82a6)

To read more click on https://medium.com/@tahera-firdose/exploring-random-forest-algorithm-from-theory-to-practice-with-python-2ab79cb43552

# Day 54 - Random Forest Classifier - HyperParameter Tuning
Hyperparameter tuning is important for algorithms. It improves their overall performance of a machine learning model and is set before the learning process and happens outside of the model. If hyperparameter tuning does not occur, the model will produce errors and inaccurate results as the loss function is not minimized.

Hyperparameter tuning is about finding a set of optimal hyperparameter values which maximizes the model's performance, minimizes loss and produces better outputs. 

To learn about the different hyper parameters and hypertune using Grid Search cv click on 

# Day 55 Support Vector Machine

What is Support Vector Machine - The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N ‚Äî the number of features) that distinctly classifies the data points.

![gif](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/ccbd89ab-4668-4787-b8f6-ad89d7257c5f)

To learn more about SVM click on https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47

# Day 56 - SVM Regressor and Hands on Practise

Support Vector Regression (SVR) is a type of machine learning algorithm used for regression analysis. The goal of SVR is to find a function that approximates the relationship between the input variables and a continuous target variable, while minimizing the prediction error.
[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/SVM)

# Day 57 - SVm Classification

Want to learn about this topic click on  https://towardsdatascience.com/machine-learning-basics-support-vector-machine-svm-classification-205ecd28a09d

[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/SVM)


# Day 58 and 59- Ensemble - Boosting
AdaBoost, short for Adaptive Boosting, stands as one of the pioneering ensemble learning algorithms

Refer to https://medium.com/@tahera-firdose/boosting-in-machine-learning-adaboost-1d83fa370588 for detailed explaination

# Day 60 Understanding Train, Test, and Validation
The Training Grounds: Where Models Grow : This is where the seeds of algorithms are sown and nurtured. It's the playground where models learn the patterns, relationships, and intricacies of the data.
The Validation Valley: Tweaking for Excellence: After training in the grounds, models visit the Validation Valley. Here, their skills are tested, weaknesses identified, and strategies adjusted.
The Test Terrain: The Ultimate Showdown: A sacred ground, where the final showdown happens. Models, after rigorous training and validation, come here to prove their worth.

Please refer to the blog for details : https://medium.com/@tahera-firdose/understanding-train-test-and-validation-data-why-they-matter-4c0d3a822904

# Day 61: Model Evaluation for Classification: Ke Metrics - Accuracy, Precision, Recall, ROC, and AUC.
Refer to blog https://medium.com/@tahera-firdose/model-evaluation-for-classification-a-deep-dive-into-key-metrics-aa4009872d9a for deeper understanding

# Day 62 Introduction to Unsupervised Learning
From here we will be starting our journey to Unsupervised Learning. Please refer to blog for understanding about Unsupervsied Learning.
https://tahera-firdose.medium.com/unsupervised-learning-unveiling-the-hidden-patterns-in-data-aca087b65e14

# Day 63 Kmeans Clustering
K-means clustering is a popular unsupervised machine learning algorithm used for partitioning data points into distinct groups or clusters. The primary objective is to group similar data points together while keeping dissimilar points in separate clusters. The ‚ÄúK‚Äù in K-means represents the number of clusters the algorithm should form.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/3cab9a54-adea-4b91-b73c-b8aa296c8180)


click on https://medium.com/@tahera-firdose/mastering-k-means-clustering-a-guide-for-beginners-8f38f1461809 to lear anout kmeans clustering in detail

# Day 64 - Understanding the Elbow Method: Finding the Optimal Number of Clusters

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/c116d770-6493-4c05-b1ae-9221f19173cd)

refer to blog to explore about Elbow method https://medium.com/@tahera-firdose/understanding-the-elbow-method-finding-the-optimal-number-of-clusters-68319d773ea3

# Day 65 Understanding the Silhouette Score

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/95995d55-4fb4-4979-98a8-3d586d90fbb6)

For a more in-depth comprehension of the Silhouette Score, you can explore the following blog: "Exploring the Silhouette Score: A Comprehensive Guide" available at this https://medium.com/@tahera-firdose/understanding-the-silhouette-score-fb5109ea28dc for deeper understanding, This resource provides a detailed and comprehensive explanation of the Silhouette Score, offering valuable insights into its significance and practical applications.

# Day 66 Hierarchiical Clustering 

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/08372712-f2e4-43d8-ab4f-386ffc4bfdb2)

For a more profound understanding of hierarchical clustering and its practical applications, you can explore the blog titled "Unlocking the Secrets of Hierarchical Clustering: Finding Patterns in Data." This resource provides comprehensive insights into hierarchical clustering techniques and guides you through the process of uncovering patterns within datasets.
https://medium.com/@tahera-firdose/unlocking-the-secrets-of-hierarchical-clustering-finding-patterns-in-data-f782c8765d9c

# Day 67 Hierarchical clustering using python 

You can find a comprehensive guide on hierarchical clustering in Python by visiting the following Medium blog:

Title: "Exploring Hierarchical Clustering in Python: Uncovering Data Structures"

Link: Hierarchical Clustering in Python - [Medium Blog](https://medium.com/@tahera-firdose/hierarchical-clustering-in-python-unraveling-data-structures-b646b5abb8df)

This blog provides an in-depth exploration of hierarchical clustering, offering insights into data structures and practical Python implementation. It covers key concepts, step-by-step instructions, and code examples to help you understand and apply hierarchical clustering in your data analysis projects.


