# 100-days-of-Machine-Learning

# Day1 
Day 1 of learning Numpy! Numpy is a powerful Python library used for numerical computing. It provides a fast and efficient way to work with arrays, and is widely used in the fields of data science, machine learning, and scientific computing.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Numpy)

# Day2 and 3 
Day 2 and 3 of learning Pandas! Pandas is a powerful Python library used for data manipulation and analysis. It provides fast, flexible, and expressive data structures designed to make working with "relational" or "labeled" data both easy and intuitive Refer book - https://github.com/shyamkumarchauhan/learn-python-with-shyam/blob/master/cheatsheet/Pandas-Cookbook-eBook.pdf.

# Day4

Some More Pandas advanced concepts multiindexing,pivottable, datetime concepts Refer to my blog on pivot table : https://medium.com/@tahera-firdose/exploring-data-with-pivot-tables-in-pandas-a-practical-guide-with-examples-from-the-titanic-938f65f64285

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Pandas)

# Day 5 
data visualization using the Matplotlib library in Python.

Matplotlib is a powerful data visualization library widely used by data scientists, analysts, and researchers to create informative and appealing visualizations. It provides a range of tools for creating various types of plots, including line charts, bar charts, scatter plots, histograms, heatmaps, and more.

The examples in this repository cover different use cases and demonstrate how to use Matplotlib to create effective and informative visualizations. The examples include:

Line Chart: A simple example of creating a line chart using Matplotlib.

Bar Chart: An example of creating a bar chart to compare data across different categories.

Scatter Plot: An example of creating a scatter plot to visualize the relationship between two variables.

Histogram: An example of creating a histogram to visualize the distribution of a dataset.

Heatmap: An example of creating a heatmap to visualize the correlation between variables in a dataset.
 
 [code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Matplotlib.ipynb)
 
 [part2](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Matplotlib%20Part2.ipynb)
# Day 6

Data Visualization using  Plotly, which is a powerful data visualization library in Python. Plotly allows us to create interactive and dynamic visualizations, making it a great tool for exploratory data analysis and storytelling.

I explore the various features and capabilities of Plotly and learnt how to create basic plots such as line plots, bar charts, scatter plots, and pie charts. Additionally, explored more advanced visualizations like heatmaps, 3D plots, and animated plots.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Plotly.ipynb)

# Day 7,8,9 

During these three days, I covered a range of SQL concepts and techniques. I started with the basics, such as creating tables, inserting data, and querying data using the SELECT statement and then progress to more advanced topics, including joins, aggregations, subqueries, and data manipulation.

I also solved some question from Leetcode. Refer to PDF for the database questions with solutions

Also followed https://www.youtube.com/@techTFQ and learnt some important concepts like subqueries, joins, windows and normalization.

# Day 10 - Data Visualiation using Tableau

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9063973e-70d4-4781-8962-a5b812e62290)

# Day 11

Feature Scaling - Learnt Feature scaling - Standardization and Normalization methods
Refer to Blog - https://medium.com/p/122bd97cdbfc

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Feature%20Scaling)
# Day 12

Categorical Data encoding - Understood the below encoding techniques for Categorical data
1. OneHot Encoding or get dummies -Used to convert Nominal categorical data to Numeric data
2. Ordinal Encoder - Used to convert Ordinal categorical data to Numeric data
3. Label Encoder - Used to convert target variables from Categorical to Numericcal Data

Please refer to https://www.linkedin.com/feed/update/urn:li:activity:7067400868047794177/ for a complete detail analysis of the categoricaal data with examples

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Categorical%20data%20Encoding)

# Day 13

Column Transformer and Pipeline
![comparision_61645596](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/92a0becd-734e-4938-8bfe-47600b309a06)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Column%20Transformer%20and%20Pipelines)

# Day 14 - Handling missing values using dropna
Handling Missing Values with dropna: I explored into the importance of addressing missing values and explore the dropna method as a popular solution. I discuss the concept of missing values, their consequences, and how the dropna method can simplify data preprocessing.

Refer to blog for complete analysis https://tahera-firdose.medium.com/the-importance-of-handling-missing-values-an-overview-of-the-dropna-method-c70795c04c2c

# Day 15 - Handling missing values using mean and median
Missing values can hinder accurate analysis and modeling, but by filling them with the mean and median, we can preserve the overall distribution of the data. In this blog post, I dive into the process, advantages, and limitations of this approach, and demonstrate how to implement it using Python.

https://www.linkedin.com/feed/update/urn:li:activity:7068816645347049472/

# Day16 - Explored Most Frequent Imputation and Missing Category Imputation for Categorical data

Most Frequent Imputation: replacing missing values with the most frequently occurring category in the variable.
Missing Category Imputation: assigning a separate category to represent missing values.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/Handling%20Categrical%20Data.ipynb)

# Day17 - Random Value Imputation

Random imputation involves replacing missing values with randomly selected values from the existing dataset. This approach is useful when the missingness is random and there is no specific pattern or correlation to the missing values. Please refer to https://www.linkedin.com/feed/

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/Random%20Imputation.ipynb)

# Day 18 - KNN Imputation

KNN Imputation: KNN imputation is a method used to fill in missing values by estimating them from neighboring data points. It is called multivariate because it takes into account multiple variables or features in the dataset to estimate missing values. By considering the values of other variables, KNN imputation leverages relationships and patterns within the data to impute missing values effectively.

For more details please refer to https://medium.com/@tahera-firdose/knn-imputation-an-effective-approach-for-handling-missing-data-5c8bbb45c81a

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/KNN%20Imputation.ipynb)

# Day 19 and 20

Outliers : Outliers are data points that significantly deviate from the majority of the dataset, potentially indicating unusual or erroneous observations. Handling outliers is crucial in data analysis and modeling to ensure accurate insights and reliable results.

![istockphoto-515066055-1024x1024](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/6bcb60df-9bc9-4f48-a4b8-873a2b43a7a7)

Refer to https://www.linkedin.com/feed/update/urn:li:activity:7070132016687558657/ for complete details.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Handling%20Outliers)

# Day 21 - Learnt key concepts about simple linear regression including the Best Fit Line, Slope, Intercept, and Residual Error.

üìà What is Simple Linear Regression?
Simple Linear Regression is a fundamental technique in statistics and machine learning used to establish a relationship between two variables: the dependent variable (y) and the independent variable (x). It aims to find the best fit line that represents the relationship between these variables.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/7614ce9e-b26f-4c55-a1b6-319931e18cce)


üìâ Best Fit Line:
The Best Fit Line, also known as the Regression Line, is the line that minimizes the overall distance between the observed data points and the predicted values. It serves as a visual representation of the relationship between the independent and dependent variables.

üìè Slope:
The Slope (m) of the Best Fit Line measures the rate of change of the dependent variable (y) with respect to the independent variable (x). It quantifies the direction and steepness of the line. A positive slope indicates a positive correlation, while a negative slope represents a negative correlation between the variables.

üéØ Intercept:
The Intercept (b) is the point where the Best Fit Line intersects the y-axis. It represents the predicted value of the dependent variable (y) when the independent variable (x) is zero. The intercept helps determine the initial starting point of the line.

üîç Residual Error:
Residual Error, also referred to as Residuals, represents the difference between the observed values and the predicted values on the Best Fit Line. It quantifies how well the regression line fits the data points. Minimizing the residual error is a crucial aspect of Simple Linear Regression.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/Simple%20LInear%20Regression%20using%20Scikit%20learn.ipynb)

# Day 22 and 23 - Understood the Geometric intuition of best fit line using OLS.

Please refer to https://tahera-firdose.medium.com/linear-regression-derivation-of-slope-and-intercept-using-ordinary-least-square-971534ec6b77 for the derivation of Slope and Intercept using OLS.

Also coded the Linear Regression class from scratch and compared the results with ScikitLearn Linear Regression class
[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/LinearRegression%20from%20Scratch.ipynb)

# Day 24 - Learnt Evaluation Metrics used in Linear Regression

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9005856d-2ce4-453a-8579-afaae56d050d)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/f35248cc-b14f-43e6-98ea-b20f1a2b8216)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/6427ebb5-e750-4e62-b447-d66fa3908122)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/c5867b2e-04e4-4f14-b3ef-1bcb9bc7a9b4)

Please refer to blog https://medium.com/@tahera-firdose/understanding-regression-evaluation-metrics-244a106ac30f

[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/Evaluation%20Metricsipynb)

# Day 25 - Multiple Linear Regression

Multiple linear regression is an extension of simple linear regression, where we aim to model the linear relationship between multiple independent variables and a dependent variable. It assumes that the relationship between the independent variables and the dependent variable is additive, meaning each independent variable contributes independently to the dependent variable.

The Equation:
In multiple linear regression, we express the relationship between the independent variables (x1, x2, ..., xn) and the dependent variable (y) through the following equation:

y = Œ≤0 + Œ≤1 * x1 + Œ≤2 * x2 + ... + Œ≤n * xn + Œµ

Here, y represents the dependent variable, Œ≤0 is the intercept, Œ≤1 to Œ≤n are the coefficients corresponding to the independent variables, x1 to xn, and Œµ represents the error term

Estimating Coefficients:
The coefficients (Œ≤0, Œ≤1, Œ≤2, ..., Œ≤n) in the multiple linear regression equation are estimated using various statistical techniques. The most common method is Ordinary Least Squares (OLS), which minimizes the sum of squared differences between the observed and predicted values.

Interpreting Coefficients:
The coefficients in multiple linear regression indicate the change in the dependent variable corresponding to a one-unit change in the respective independent variable, holding other variables constant. Positive coefficients suggest a positive relationship, while negative coefficients suggest a negative relationship.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9cbfed98-a7e8-46cf-b0b9-f6018ca5c87d)



[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Multiple%20Linear%20Regression)

# Day 26 and Day 27 - Gradient Descent

Gradient descent is an iterative process that helps us find the minimum or maximum point of a function. The "gradient" refers to the slope or direction of change of the function, much like the steepest downhill direction on a hilly terrain. The "descent" part means moving in the direction opposite to the gradient, taking small steps toward the bottom of the hill. By repeatedly adjusting our position based on the negative gradient, we gradually approach the minimum (or maximum) point, making our way down the slope and finding the optimal value of the function.


![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/f901b703-f278-4e13-88e5-1f630a96b988)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Gradient%20Descent) 

# Day 28 and Day 29 - Learnt About different types of Gradient Descent

**Batch Gradient Descent (BGD)** computes the gradient using the entire training dataset, ensuring stability but being computationally expensive for large datasets.

![Batch G](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/87f007c1-b4d1-4978-84f0-37f6bcb61031)

**Stochastic Gradient Descent (SGD)** takes it further by using one training example at a time, offering efficiency and the potential to find better solutions but with more erratic convergence.

![Stochastic G](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/a5bc1368-c3d9-4600-bb03-23ce2031b555)

**Mini-Batch Gradient Descent (MBGD)** strikes a balance by using subsets of the data, providing faster convergence and parallelization.

![Mini Batch G](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/7847440a-fb62-4484-b72a-716d80b9fb79)

# Day 30 Polynomial Regression

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/5a6941cd-7e79-48e6-8760-70a6b615dd4d)

Refer to blog https://tahera-firdose.medium.com/understanding-polynomial-regression-603eb25501d

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Polynomial%20Regression)

# Day 31 Local and Global Minima

Local Minima: In the context of optimization, a local minimum refers to a point or value in a function where the function reaches the lowest value within a specific region but may not be the lowest value across the entire function.

Global Minima: In optimization, a global minimum is the absolute lowest point or value in a function, meaning it is the lowest value across the entire function domain. It represents the global "valley" or the optimal solution that yields the lowest possible value.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/881f4483-82f4-458a-a041-ccb91a7f3f7a)

![local_minima_vs_global_minima](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/991dc121-de3f-4993-9403-eaa90b5c95cf)

# Day 32 - Overfitting, Underfitting, Bias Variance Tradeoff

Finding the optimal balance between the bias and Variance  is the key to unlocking model performance and generalization. Refer to blog for enhancing your understanding of this fundamental concept in AI.
https://medium.com/@tahera-firdose/bias-variance-tradeoffs-61ea08a25f6f

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/e2387f33-ba6b-4224-b1ab-b321692fe5db)

# Day 33 Ridge Regression using OLS

 Ridge regression modifies the OLS estimation by introducing a penalty term that controls the magnitude of the coefficients. This tradeoff between fitting the data and shrinking the coefficients helps to find a balance between model complexity and generalization, making Ridge regression a useful technique for handling multicollinearity and reducing model variance.

 ![Ridge-regression-variable-selection](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/d2e340aa-a5a4-4019-a3e3-d85c75fb7152)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Ridge%20Regression/Ridge%20Regression%20-%20OLS.ipynb) 
 # Day 34 Ridge Regression using Gradient Descent

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/81564254-7bf1-481c-8371-92f42298cd9d)

Ridge regression using gradient descent is an iterative optimization algorithm that iteratively updates the coefficients by taking steps in the direction of the steepest descent of the objective function. It is a numerical approximation method that can handle large datasets and high-dimensional problems more efficiently. Gradient descent allows for fine-tuning the learning rate and regularization parameter to control the convergence speed and balance between fitting the data and regularization.

The key steps involved in Ridge regression using gradient descent are as follows:

Initialize the coefficients.

Calculate the gradient of the objective function with respect to the coefficients.

Update the coefficients by taking steps in the direction of the negative gradient, multiplied by the learning rate.

Repeat steps 2 and 3 until convergence or a maximum number of iterations is reached.

Ridge regression using gradient descent offers more flexibility and scalability compared to the OLS approach. It allows for efficient optimization and can handle larger and more complex datasets. However, it is an approximation method and might require careful tuning of hyperparameters such as the learning rate and regularization parameter to achieve optimal results.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Ridge%20Regression/Ridge%20_%20Gradient%20Descent.ipynb)

# Day 35 Lasso L1 Regression  

Lasso (Least Absolute Shrinkage and Selection Operator) is a regression technique that performs feature selection and regularization by shrinking less important feature coefficients to zero, leading to sparsity in the model.

Refer to blog for detailed understanding

https://tahera-firdose.medium.com/lasso-regression-a-comprehensive-guide-to-feature-selection-and-regularization-2c6a20b61e23

# Day 36 Elastic Net regression

Elastic Net regression is a powerful technique that combines the benefits of L1 (Lasso) and L2 (Ridge) regularization methods. It addresses some of the limitations of these individual regularization techniques and offers a flexible approach for handling high-dimensional datasets with potential multicollinearity issues.

The mathematical formulation of Elastic Net regression can be represented as follows:

Minimize:

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/041a0ca3-c80d-4193-8b6d-59bbc72a1d73)

Refer hto blog to learn more on ttps://www.linkedin.com/advice/0/what-some-common-pitfalls-challenges-elastic#:~:text=What%20is%20elastic%20net%20regression,by%20a%20parameter%20called%20alpha.

# Day 37 Logistic Regression - Sigmoid Function

Please refer to blog to understand the sigmoid function 

https://www.learndatasci.com/glossary/sigmoid-function/

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/1d6766f7-a3a5-43bf-b343-02dd2ad09f6b)

# Day 38 Logistic Regression - Loss Functiion

Refer to blog to understand the loss function for Logistic regresion

https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11

# Day 39 Log Loss error

Refer to blog for details
https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a

# Day 40 Binary Classification 
Binary classification is a type of machine learning task that involves categorizing data into one of two classes or categories. The goal of binary classification is to develop a model that can learn from labeled examples and make predictions on new, unseen examples.

In binary classification, the two classes are often referred to as the positive class and the negative class, or class 1 and class 0. The model learns patterns and relationships in the input data to determine which class a new example belongs to. The input data is typically represented by a set of features or attributes that describe each example.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/31ec4432-5822-4366-9f35-c86b1f53b249)

Refer to blog: https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/

# Day 41 Precision, Recall, F1 score, Confusion Matrix

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/5642ff2a-b62c-44cd-8b11-f361ed6a3d23)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/97f3a840-16dd-4167-aae1-4ec6b3d2331d)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9fd75220-850e-45c2-a2b7-c733de8b31ee)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/a3b96f2a-ae5a-48ab-a8ed-f8561b943fa9)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/83cd0066-7992-4c94-9381-b96a202bfa87)


Please refer to blog to gain the insights on this topic

https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-

https://proclusacademy.com/blog/explainer/precision-recall-f1-score-classification-models/

https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/

# Day 42 - Multiclass - Softmax regression

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/49456713-c1dc-4da4-b0de-dce9661be301)

Refer to video https://www.youtube.com/watch?v=hYBwBmojXoU to learn about Softmax Regression


