# 100-days-of-Machine-Learning

# Day1 
Day 1 of learning Numpy! Numpy is a powerful Python library used for numerical computing. It provides a fast and efficient way to work with arrays, and is widely used in the fields of data science, machine learning, and scientific computing.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Numpy)

# Day2 and 3 
Day 2 and 3 of learning Pandas! Pandas is a powerful Python library used for data manipulation and analysis. It provides fast, flexible, and expressive data structures designed to make working with "relational" or "labeled" data both easy and intuitive Refer book - https://github.com/shyamkumarchauhan/learn-python-with-shyam/blob/master/cheatsheet/Pandas-Cookbook-eBook.pdf.

# Day4

Some More Pandas advanced concepts multiindexing,pivottable, datetime concepts Refer to my blog on pivot table : https://medium.com/@tahera-firdose/exploring-data-with-pivot-tables-in-pandas-a-practical-guide-with-examples-from-the-titanic-938f65f64285

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Pandas)

# Day 5 
data visualization using the Matplotlib library in Python.

Matplotlib is a powerful data visualization library widely used by data scientists, analysts, and researchers to create informative and appealing visualizations. It provides a range of tools for creating various types of plots, including line charts, bar charts, scatter plots, histograms, heatmaps, and more.

The examples in this repository cover different use cases and demonstrate how to use Matplotlib to create effective and informative visualizations. The examples include:

Line Chart: A simple example of creating a line chart using Matplotlib.

Bar Chart: An example of creating a bar chart to compare data across different categories.

Scatter Plot: An example of creating a scatter plot to visualize the relationship between two variables.

Histogram: An example of creating a histogram to visualize the distribution of a dataset.

Heatmap: An example of creating a heatmap to visualize the correlation between variables in a dataset.
 
 [code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Matplotlib.ipynb)
 
 [part2](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Matplotlib%20Part2.ipynb)
# Day 6

Data Visualization using  Plotly, which is a powerful data visualization library in Python. Plotly allows us to create interactive and dynamic visualizations, making it a great tool for exploratory data analysis and storytelling.

I explore the various features and capabilities of Plotly and learnt how to create basic plots such as line plots, bar charts, scatter plots, and pie charts. Additionally, explored more advanced visualizations like heatmaps, 3D plots, and animated plots.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/DataVisualization/Plotly.ipynb)

# Day 7,8,9 

During these three days, I covered a range of SQL concepts and techniques. I started with the basics, such as creating tables, inserting data, and querying data using the SELECT statement and then progress to more advanced topics, including joins, aggregations, subqueries, and data manipulation.

I also solved some question from Leetcode. Refer to PDF for the database questions with solutions

Also followed https://www.youtube.com/@techTFQ and learnt some important concepts like subqueries, joins, windows and normalization.

# Day 10 - Data Visualiation using Tableau

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9063973e-70d4-4781-8962-a5b812e62290)

# Day 11

Feature Scaling - Learnt Feature scaling - Standardization and Normalization methods
Refer to Blog - https://medium.com/p/122bd97cdbfc

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Feature%20Scaling)
# Day 12

Categorical Data encoding - Understood the below encoding techniques for Categorical data
1. OneHot Encoding or get dummies -Used to convert Nominal categorical data to Numeric data
2. Ordinal Encoder - Used to convert Ordinal categorical data to Numeric data
3. Label Encoder - Used to convert target variables from Categorical to Numericcal Data

Please refer to https://www.linkedin.com/feed/update/urn:li:activity:7067400868047794177/ for a complete detail analysis of the categoricaal data with examples

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Categorical%20data%20Encoding)

# Day 13

Column Transformer and Pipeline
![comparision_61645596](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/92a0becd-734e-4938-8bfe-47600b309a06)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Column%20Transformer%20and%20Pipelines)

# Day 14 - Handling missing values using dropna
Handling Missing Values with dropna: I explored into the importance of addressing missing values and explore the dropna method as a popular solution. I discuss the concept of missing values, their consequences, and how the dropna method can simplify data preprocessing.

Refer to blog for complete analysis https://tahera-firdose.medium.com/the-importance-of-handling-missing-values-an-overview-of-the-dropna-method-c70795c04c2c

# Day 15 - Handling missing values using mean and median
Missing values can hinder accurate analysis and modeling, but by filling them with the mean and median, we can preserve the overall distribution of the data. In this blog post, I dive into the process, advantages, and limitations of this approach, and demonstrate how to implement it using Python.

https://www.linkedin.com/feed/update/urn:li:activity:7068816645347049472/

# Day16 - Explored Most Frequent Imputation and Missing Category Imputation for Categorical data

Most Frequent Imputation: replacing missing values with the most frequently occurring category in the variable.
Missing Category Imputation: assigning a separate category to represent missing values.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/Handling%20Categrical%20Data.ipynb)

# Day17 - Random Value Imputation

Random imputation involves replacing missing values with randomly selected values from the existing dataset. This approach is useful when the missingness is random and there is no specific pattern or correlation to the missing values. Please refer to https://www.linkedin.com/feed/

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/Random%20Imputation.ipynb)

# Day 18 - KNN Imputation

KNN Imputation: KNN imputation is a method used to fill in missing values by estimating them from neighboring data points. It is called multivariate because it takes into account multiple variables or features in the dataset to estimate missing values. By considering the values of other variables, KNN imputation leverages relationships and patterns within the data to impute missing values effectively.

For more details please refer to https://medium.com/@tahera-firdose/knn-imputation-an-effective-approach-for-handling-missing-data-5c8bbb45c81a

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Handling%20Missing%20Data/KNN%20Imputation.ipynb)

# Day 19 and 20

Outliers : Outliers are data points that significantly deviate from the majority of the dataset, potentially indicating unusual or erroneous observations. Handling outliers is crucial in data analysis and modeling to ensure accurate insights and reliable results.

![istockphoto-515066055-1024x1024](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/6bcb60df-9bc9-4f48-a4b8-873a2b43a7a7)

Refer to https://www.linkedin.com/feed/update/urn:li:activity:7070132016687558657/ for complete details.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Handling%20Outliers)

# Day 21 - Learnt key concepts about simple linear regression including the Best Fit Line, Slope, Intercept, and Residual Error.

üìà What is Simple Linear Regression?
Simple Linear Regression is a fundamental technique in statistics and machine learning used to establish a relationship between two variables: the dependent variable (y) and the independent variable (x). It aims to find the best fit line that represents the relationship between these variables.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/7614ce9e-b26f-4c55-a1b6-319931e18cce)


üìâ Best Fit Line:
The Best Fit Line, also known as the Regression Line, is the line that minimizes the overall distance between the observed data points and the predicted values. It serves as a visual representation of the relationship between the independent and dependent variables.

üìè Slope:
The Slope (m) of the Best Fit Line measures the rate of change of the dependent variable (y) with respect to the independent variable (x). It quantifies the direction and steepness of the line. A positive slope indicates a positive correlation, while a negative slope represents a negative correlation between the variables.

üéØ Intercept:
The Intercept (b) is the point where the Best Fit Line intersects the y-axis. It represents the predicted value of the dependent variable (y) when the independent variable (x) is zero. The intercept helps determine the initial starting point of the line.

üîç Residual Error:
Residual Error, also referred to as Residuals, represents the difference between the observed values and the predicted values on the Best Fit Line. It quantifies how well the regression line fits the data points. Minimizing the residual error is a crucial aspect of Simple Linear Regression.

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/Simple%20LInear%20Regression%20using%20Scikit%20learn.ipynb)

# Day 22 and 23 - Understood the Geometric intuition of best fit line using OLS.

Please refer to https://tahera-firdose.medium.com/linear-regression-derivation-of-slope-and-intercept-using-ordinary-least-square-971534ec6b77 for the derivation of Slope and Intercept using OLS.

Also coded the Linear Regression class from scratch and compared the results with ScikitLearn Linear Regression class
[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/LinearRegression%20from%20Scratch.ipynb)

# Day 24 - Learnt Evaluation Metrics used in Linear Regression

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9005856d-2ce4-453a-8579-afaae56d050d)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/f35248cc-b14f-43e6-98ea-b20f1a2b8216)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/6427ebb5-e750-4e62-b447-d66fa3908122)

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/c5867b2e-04e4-4f14-b3ef-1bcb9bc7a9b4)

Please refer to blog https://medium.com/@tahera-firdose/understanding-regression-evaluation-metrics-244a106ac30f

[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/blob/master/Simple%20Linear%20Regression/Evaluation%20Metricsipynb)

# Day 25 - Multiple Linear Regression

Multiple linear regression is an extension of simple linear regression, where we aim to model the linear relationship between multiple independent variables and a dependent variable. It assumes that the relationship between the independent variables and the dependent variable is additive, meaning each independent variable contributes independently to the dependent variable.

The Equation:
In multiple linear regression, we express the relationship between the independent variables (x1, x2, ..., xn) and the dependent variable (y) through the following equation:

y = Œ≤0 + Œ≤1 * x1 + Œ≤2 * x2 + ... + Œ≤n * xn + Œµ

Here, y represents the dependent variable, Œ≤0 is the intercept, Œ≤1 to Œ≤n are the coefficients corresponding to the independent variables, x1 to xn, and Œµ represents the error term

Estimating Coefficients:
The coefficients (Œ≤0, Œ≤1, Œ≤2, ..., Œ≤n) in the multiple linear regression equation are estimated using various statistical techniques. The most common method is Ordinary Least Squares (OLS), which minimizes the sum of squared differences between the observed and predicted values.

Interpreting Coefficients:
The coefficients in multiple linear regression indicate the change in the dependent variable corresponding to a one-unit change in the respective independent variable, holding other variables constant. Positive coefficients suggest a positive relationship, while negative coefficients suggest a negative relationship.

![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/9cbfed98-a7e8-46cf-b0b9-f6018ca5c87d)



[Code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Multiple%20Linear%20Regression)

#Day 26 and Day 27 - Gradient Descent

Gradient descent is an iterative process that helps us find the minimum or maximum point of a function. The "gradient" refers to the slope or direction of change of the function, much like the steepest downhill direction on a hilly terrain. The "descent" part means moving in the direction opposite to the gradient, taking small steps toward the bottom of the hill. By repeatedly adjusting our position based on the negative gradient, we gradually approach the minimum (or maximum) point, making our way down the slope and finding the optimal value of the function.


![image](https://github.com/taherafirdose/100-days-of-Machine-Learning/assets/66288578/f901b703-f278-4e13-88e5-1f630a96b988)

[code](https://github.com/taherafirdose/100-days-of-Machine-Learning/tree/master/Gradient%20Descent) 

